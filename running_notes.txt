*************************************************************************************
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb

dict_classifiers = {
  "Logistic Regression": {
    'classifier': LogisticRegression(),
    'params': [{
      'penalty': ['l1', 'l2'],
      'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],
      'solver': ['lbfgs', 'sag', 'saga', 'liblinear']
    }]
  },
  "Nearest Neighbors": {
    'classifier': KNeighborsClassifier(),
    'params': [{
      'n_neighbors': [1, 3, 5, 10],
      'leaf_size': [3, 30]
    }]
  },

  "SVM": {
    'classifier': SVC(),
    'params': [{
      'C': [1, 10, 100, 1000],
      'gamma': [0.001, 0.0001],
      'kernel': ['linear']
    }]
  },
  "Linear SVM": {
    'classifier': LinearSVC(),
    'params': {}
  },
  "Gradient Boosting Classifier": {
    'classifier': GradientBoostingClassifier(),
    'params': [{
      'learning_rate': [0.05, 0.1],
      'n_estimators': [50, 100, 200],
      'max_depth': [3, None]
    }]
  },
  "Decision Tree": {
    'classifier': tree.DecisionTreeClassifier(),
    'params': [{
      'max_depth': [3, None]
    }]
  },
  "Random Forest": {
    'classifier': RandomForestClassifier(),
    'params': {}
  },
  "Naive Bayes": {
    'classifier': GaussianNB(),
    'params': {}
  },
  "XGBoost": {
    'classifier': xgb.XGBClassifier(),
    'params': {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['binary:logistic','multi:softmax'],
              'learning_rate': [0.01,0.05,0.07,0.1], #so called `eta` value
              'max_depth': [3,5,7,9],
              'min_child_weight': [2,5,8,11,13,17],
              'silent': [0],
              'subsample': [0.5,0.7,0.9],
              'colsample_bytree': [0.5,0.7,0.9],
              'n_estimators': [5,10,20,50,70,100,300,700,1000], #number of trees, change it to 1000 for better results
              'missing':[-999],
              'seed': [1337]
              }
  }
}


from sklearn.preprocessing import StandardScaler

data_train = StandardScaler().fit_transform(data_train)
data_test = StandardScaler().fit_transform(data_test)

import time
from sklearn.model_selection import GridSearchCV 
from sklearn.metrics import accuracy_score
num_classifiers = len(dict_classifiers.keys())

def batch_classify(X_train, Y_train, X_test, Y_test):
    df_results = pd.DataFrame(
        data=np.zeros(shape=(num_classifiers,4)),
        columns = ['classifier',
                   'train_score', 
                   'test_score',
                   'training_time'])
    count = 0
    for key, classifier in dict_classifiers.items():
        t_start = time.time()
        grid = GridSearchCV(classifier['classifier'], 
                      classifier['params'],
                      refit=True,
                        cv = 10,
                        scoring = 'accuracy', # scoring metric
                        n_jobs = -1
                        )
        estimator = grid.fit(X_train,Y_train)
        t_end = time.time()
        t_diff = t_end - t_start
        train_score = estimator.score(X_train,Y_train)
        test_score = estimator.score(X_test,Y_test)
        df_results.loc[count,'classifier'] = key
        df_results.loc[count,'train_score'] = train_score
        df_results.loc[count,'test_score'] = test_score
        df_results.loc[count,'training_time'] = t_diff
        count+=1
    return df_results


df_results = batch_classify(data_train, label_train, data_test, label_test)
display(df_results.sort_values(by='test_score', ascending=False))

*************************************************************************************
